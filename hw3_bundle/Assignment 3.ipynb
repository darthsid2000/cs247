{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS247 Advanced Data Mining - Assignment 3\n",
    "## Deadline: 11:59PM, February 7, 2023\n",
    "\n",
    "## Instructions\n",
    "Each assignment is structured as a Jupyter notebook, offering interactive tutorials that align with our lectures. You will encounter two types of problems: *write-up problems* and *coding problems*.\n",
    "\n",
    "1. **Write-up Problems:** These problems are primarily theoretical, requiring you to demonstrate your understanding of lecture concepts and to provide mathematical proofs for key theorems. Your answers should include sufficient steps for the mathematical derivations.\n",
    "2. **Coding Problems:** Here, you will be engaging with practical coding tasks. These may involve completing code segments provided in the notebooks or developing models from scratch.\n",
    "\n",
    "To ensure clarity and consistency in your submissions, please adhere to the following guidelines:\n",
    "\n",
    "* For write-up problems, use Markdown bullet points to format text answers. Also, express all mathematical equations using $\\LaTeX$ and avoid plain text such as `x0`, `x^1`, or `R x Q` for equations.\n",
    "* For coding problems, comment on your code thoroughly for readability and ensure your code is executable. Non-runnable code may lead to a loss of **all** points. Coding problems have automated grading, and altering the grading code will result in a deduction of **all** points.\n",
    "* Your submission should show the entire process of data loading, preprocessing, model implementation, training, and result analysis. This can be achieved through a mix of explanatory text cells, inline comments, intermediate result displays, and experimental visualizations.\n",
    "\n",
    "### Submission Requirements\n",
    "\n",
    "* Submit your solutions through GradeScope in BruinLearn.\n",
    "* Late submissions are allowed up to 24 hours post-deadline with a penalty factor of $\\mathbf{1}(t\\leq24)e^{-(\\ln(2)/12)t}$.\n",
    "\n",
    "### Collaboration and Integrity\n",
    "\n",
    "* Collaboration is encouraged, but all final submissions must be your own work. Please acknowledge any collaboration or external sources used, including websites, papers, and GitHub repositories.\n",
    "* Any suspicious cases of academic misconduct will be reported to The Office of the Dean of Students.\n",
    "\n",
    "## Outline\n",
    "* Part 1: Topic Model Continued\n",
    "* Part 2: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Support\n",
    "\n",
    "Considering the size of the training data, it is strongly suggested to use [Google Colab](https://colab.research.google.com/) or a GPU server for this exercise. If you are using Colab, you can manually switch to a CPU device on Colab by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif USE_GPU and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Topic Models Continued (35 points)\n",
    "\n",
    "Please see the attached PDF for the write-up problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Language Modeling (65 points + 20 bonus points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Skip-Gram and Word2Vec (55 points + 10 bonus points)\n",
    "\n",
    "The first section of this exercise is to implement and analyze a Skip-Gram model for language modeling, with a focus on understanding and experimenting with negative sampling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup\n",
    "Install the following packages:\n",
    "* `torch`\n",
    "* `torchdata`\n",
    "* `torchtext`\n",
    "* `portalocker`\n",
    "* `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c pytorch 'torchtext>=0.16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U nltk\n",
    "%pip install -U 'portalocker>=2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import collections\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Data Preprocessing (10 points)\n",
    "\n",
    "Data preprocessing is a crucial step in any machine learning task. For the Skip-Gram model, the main goal is to convert raw text into a format that can be fed into the neural network. This involves loading the dataset, tokenizing the text, building a vocabulary, and generating context-target pairs.\n",
    "* Tokenization: Convert the raw text into tokens (words).\n",
    "* Vocabulary Building: Create a mapping of unique words to integers (word indices).\n",
    "* Context-Target Pair Generation: For each word in the text, you will generate context-target pairs based on a specified window size. For example, with a window size of 2, for each word, the two words before and after become its context.\n",
    "\n",
    "We will use the AG News dataset, a collection of news articles categorized into four classes. For our purpose, we are only interested in the textual content.\n",
    "The following cell has already completed the first two steps for you. You will need to complete the third step to generate context-target pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, window_size=2):\n",
    "        self.tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "        self.vocab = self.build_vocab(self.tokenized_texts)\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab.keys())}\n",
    "        self.idx_to_word = {i: word for word, i in self.word_to_idx.items()}\n",
    "        self.data = self.generate_training_data(window_size)\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        # Build a vocabulary with word frequencies for dataset\n",
    "        vocab = collections.Counter()\n",
    "        for text in texts:\n",
    "            vocab.update(text)\n",
    "        # Filter out words that appear less than 5 times\n",
    "        vocab = {word: freq for word, freq in vocab.items() if freq > 5}\n",
    "        return vocab\n",
    "\n",
    "    def generate_training_data(self, window_size):\n",
    "        # Build training data by scanning through each sentence in the dataset\n",
    "        # For each word, we build a (center_word, context_word) pair if the context_word falls within the window_size window\n",
    "        # Please note that we will skip the token that is not within `self.vocab`\n",
    "        \n",
    "        # TODO: Implement this function to generate a list of (center_word, context_word) tuples\n",
    "        training_data = []\n",
    "        \n",
    "        return training_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center_word, context_word = self.data[idx]\n",
    "        return torch.tensor(center_word), torch.tensor(context_word)\n",
    "\n",
    "news = AG_NEWS(split='test')\n",
    "dataset = CustomDataset([text for _, text in news])\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the dataset is constructed as expected\n",
    "for i, data in enumerate(dataset):\n",
    "    center_word, context_word = data\n",
    "    print(dataset.idx_to_word[center_word.item()], '->', dataset.idx_to_word[context_word.item()])\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execrise 2 (5 points)\n",
    "In the above code, we only consider very simple tokenization of the training corpus.\n",
    "In fact, there are many other ways to tokenize the text, such as [stemming](https://en.wikipedia.org/wiki/Stemming) and [lemmatization](https://en.wikipedia.org/wiki/Lemmatization). Explain the reason why or why not we should use stemming and/or lemmatization in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Model Implementation without Negative Sampling (4 points)\n",
    "\n",
    "The Skip-Gram model takes a target word as input and tries to predict its context words.\n",
    "In this exercise, please implement the Skip-Gram model using PyTorch. The model should have an [`torch.nn.embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer to convert word indices to dense vectors and a [`torch.nn.linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer to predict context words.\n",
    "Then, implement the `forward` function to define how data will pass through your model. It involves taking input words, transforming them through the embedding layer, and then making predictions using the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # TODO: Implement the Skip-Gram model architecture\n",
    "        self.embeddings = None\n",
    "        self.linear = None\n",
    "        \n",
    "\n",
    "    def forward(self, center_words):\n",
    "        out = None\n",
    "        # TODO: Implement the forward pass of the Skip-Gram model\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Model Evaluation by Finding Similar Words (5 points)\n",
    "\n",
    "Before implementing the training procedure, we can think about evaluating the model by finding similar words. The similarity between two words can be measured by the cosine similarity between their embeddings. The higher the cosine similarity, the more similar the two words are. For example, the cosine similarity between \"cat\" and \"dog\" should be higher than the cosine similarity between \"cat\" and \"car\".\n",
    "\n",
    "In this exercise, you will implement a function to find the top-k most similar words to a given word. You will need to compute the cosine similarity between the given word and all other words in the vocabulary. Then, you will return the top-k words with the highest cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def find_most_similar(word, embedding_matrix, word_to_idx, idx_to_word, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the top_k most similar words to the given word based on cosine similarity\n",
    "    :param word: The word to query\n",
    "    :param embedding_matrix: The embedding matrix to query\n",
    "    :param word_to_idx: The word-to-index mapping\n",
    "    :param idx_to_word: The index-to-word mapping\n",
    "    :param top_k: The number of most similar words to return\n",
    "\n",
    "    :return: A list of (word, similarity) pairs of length top_k, sorted by the similarity in descending order\n",
    "            Each pair denotes the most similar word to the given word and their cosine similarity.\n",
    "    \"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        return f'Word {word} not in vocabulary!'\n",
    "\n",
    "    word_idx = torch.tensor([word_to_idx[word]]).to(device)\n",
    "    word_embedding = embedding_matrix(word_idx)\n",
    "    similar_words = []\n",
    "    # TODO: Calculate the cosine similarity between the query word and all words in the vocabulary\n",
    "\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function, you can run the following cell to find the top-5 most similar words to \"Basketball\".\n",
    "Note that since the model is not trained yet, the results should be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, embedding_matrix, valid_word):\n",
    "    similar_words = find_most_similar(valid_word, embedding_matrix, dataset.word_to_idx, dataset.idx_to_word)\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Words similar to '{valid_word}':\")\n",
    "    for i, (word, similarity) in enumerate(similar_words):\n",
    "        print(f'  {i + 1}. {word}: {similarity * 100:.2f}%')\n",
    "\n",
    "embedding_dim = 100\n",
    "vocab_size = len(dataset.vocab)\n",
    "skipgram_model = SkipGramModel(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "evaluate(skipgram_model, skipgram_model.embeddings, 'basketball')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides word similarity, we can also quantitatively evaluate the model on some downstream tasks. Here, we adopt the WordSim-353 dataset, which contains 353 pairs of words and their similarity scores. The similarity scores are human-annotated and range from 0 to 10.\n",
    "We can use the cosine similarity between the embeddings of two words as the predicted similarity score. Then, we can compute the Spearman's rank correlation coefficient between the predicted and the human-annotated similarity scores. The higher the Spearman's rank correlation coefficient, the better the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word, embeddings, word_to_idx):\n",
    "    idx = word_to_idx.get(word)\n",
    "    if idx is not None:\n",
    "        idx = torch.tensor([idx]).to(device)\n",
    "        return embeddings(idx).detach().cpu().numpy()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def wordsim(embedding_matrix):\n",
    "    wordsim_data = pd.read_csv('wordsim.txt', sep='\\t', header=None, names=['Word 1', 'Word 2', 'Similarity'])\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    cosine_similarities = []\n",
    "    for _, row in wordsim_data.iterrows():\n",
    "        word1, word2, human_score = row['Word 1'], row['Word 2'], row['Similarity']\n",
    "        embedding1 = get_embedding(word1, embedding_matrix, dataset.word_to_idx)\n",
    "        embedding2 = get_embedding(word2, embedding_matrix, dataset.word_to_idx)\n",
    "\n",
    "        if embedding1 is not None and embedding2 is not None:\n",
    "            sim = cos_sim(embedding1, embedding2)[0][0]\n",
    "            cosine_similarities.append((human_score, sim))\n",
    "\n",
    "    # Calculate Spearman's rank correlation\n",
    "    human_scores, model_scores = zip(*cosine_similarities)\n",
    "    correlation, _ = spearmanr(human_scores, model_scores)\n",
    "    print(f'Spearman correlation: {correlation}')\n",
    "\n",
    "wordsim(skipgram_model.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: Model Training (5 points)\n",
    "\n",
    "Implement the `train_skipgram` function to train the model. The function should take a batch of context-target pairs as input and return the loss. You will need to compute the objective function using the negative log-likelihood loss. Then, you will need to update the model parameters using the optimizer.\n",
    "Note that in PyTorch, [`CrossEntropyLoss` combines `LogSoftmax` and `NLLLoss` in a single class](https://towardsdatascience.com/cross-entropy-negative-log-likelihood-and-all-that-jazz-47a95bd2e81). Therefore, you can directly use `CrossEntropyLoss` to compute the negative log-likelihood loss.\n",
    "\n",
    "Here we use the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with a learning rate of 0.01. During last week's discussions, we mentioned the importance of choosing appropriate optimizers and learning rates. In practice, the Adam optimizer is a good choice for many tasks, while SGD can lead to better convergence in some cases. However, the choice of learning rate can be more tricky. In general, a learning rate that is too small will lead to slow convergence, while a learning rate that is too large will lead to divergence. In this exercise, you can use the default learning rate of 0.01. However, you are encouraged to experiment with different optimizers and/or learning rates and observe the impact on the training process.\n",
    "\n",
    "For every 5 epochs, you will need to evaluate the model by finding the top-5 most similar words to \"Basketball\". You should observe that as the training progresses, the model is able to find more relevant words for \"Basketball\". On a GPU server, it should take about 10 minutes to finish training for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(skipgram_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram(model, data_loader, epochs, optimizer):\n",
    "    # TODO: Define the loss function\n",
    "    criterion = None\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in data_loader:\n",
    "            center_word, context_word = data\n",
    "            center_word, context_word = center_word.to(device), context_word.to(device)\n",
    "\n",
    "            # TODO: Implement the training pass for the Skip-Gram model\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "            evaluate(model, model.embeddings, 'basketball')\n",
    "\n",
    "train_skipgram(skipgram_model, dataloader, epochs=50, optimizer=optimizer)\n",
    "wordsim(skipgram_model.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6: Model Implementation with Negative Sampling (10 points)\n",
    "\n",
    "Negative sampling is a technique to reduce the computation time in training by altering the objective to distinguish a target word from randomly sampled \"negative\" words instead of predicting the next word directly.\n",
    "\n",
    "In this exercise, you will modify the earlier Skip-Gram model to incorporate negative sampling.\n",
    "The new model should have two embedding layers: one for the target words and one for the context words.\n",
    "For negative sampling, we consider the simplest case where the number of negative samples is fixed to 10.\n",
    "For each target word, we will randomly sample 10 words from the vocabulary as negative samples.\n",
    "Then, implement the `forward` function to define how data will pass through your model. It involves taking input words, transforming them through the embedding layers, and then computing the loss as the following equation:\n",
    "$$\n",
    "\\mathcal{L} = -\\log\\sigma(\\mathbf{u}_t^\\top\\mathbf{v}_c) - \\sum_{i=1}^k\\log\\sigma(-\\mathbf{u}_t^\\top\\mathbf{v}_{n_i}),\n",
    "$$\n",
    "where $\\mathbf{u}_t$ is the embedding of the center word, $\\mathbf{v}_c$ is the embedding of the context word (positive sample), $\\mathbf{v}_{n_i}$ is the embedding of the $i$-th negative sample, and $\\sigma$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramNegSampling, self).__init__()\n",
    "        # TODO: Define the Embedding layers for center words and context words\n",
    "        self.center_embeddings = None\n",
    "        self.context_embeddings = None\n",
    "\n",
    "    def forward(self, center_words, context_words, negative_words):\n",
    "        \"\"\"\n",
    "        Skip-Gram with Negative Sampling\n",
    "        :param center_words: the center words of shape [batch_size]\n",
    "        :param context_words: the context words of shape [batch_size]\n",
    "        :param negative_words: the negative words of shape [batch_size, num_negative_samples]\n",
    "        :return: the loss, a PyTorch scalar\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass for Skip-Gram with Negative Sampling\n",
    "        center_embeds = self.center_embeddings(center_words)\n",
    "        context_embeds = self.context_embeddings(context_words)\n",
    "        negative_embeds = self.context_embeddings(negative_words)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Negative samples can be generated in various ways, here's a simple random approach\n",
    "def get_negative_samples(batch_size, vocab_size, num_negative_samples):\n",
    "    return torch.randint(vocab_size, (batch_size, num_negative_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should follow the same training procedure as before. However, you will need to modify the `train_with_neg_sampling` function to generate context-target pairs with negative samples. You will also need to modify the `train_with_neg_sampling` function to compute the loss using the new objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_neg_sampling(model, data_loader, optimizer, epochs, vocab_size, num_negative_samples):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for center, context in data_loader:\n",
    "            negative = get_negative_samples(center.size(0), vocab_size, num_negative_samples).to(device)\n",
    "            center, context = center.to(device), context.to(device)\n",
    "\n",
    "            # TODO: Implement the training pass for the Skip-Gram model with Negative Sampling\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "            evaluate(model, model.center_embeddings, 'basketball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sampling_model = SkipGramNegSampling(vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(neg_sampling_model.parameters(), lr=learning_rate)\n",
    "train_with_neg_sampling(neg_sampling_model, dataloader, optimizer, epochs=50, vocab_size=vocab_size, num_negative_samples=10)\n",
    "wordsim(neg_sampling_model.center_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7: Exploring Different Negative Distributions in Word2Vec (16 points + 10 bonus points)\n",
    "\n",
    "Negative sampling is a crucial technique in training word embeddings. It involves selecting \"negative\" samples (words not in the target context) to train the model more efficiently. The choice of distribution from which these negative samples are drawn can significantly affect the quality of the learned embeddings.\n",
    "\n",
    "Your task is to experiment with different negative sampling distributions and analyze their impact on the model's performance. Below are several distributions you should consider:\n",
    "- Frequency-based Distribution: Words are sampled according to their frequency in the corpus. Common approaches include sampling words with a probability proportional to their frequency or their frequency raised to 3/4th power (as used in Word2Vec).\n",
    "- Subsampling of Frequent Words: Modify the frequency-based distribution to reduce the representation of extremely frequent words. This can help in reducing the bias towards very common words in the corpus.\n",
    "- Custom Distribution based on Part-of-Speech Tags: Create a distribution where the probability of a word being sampled as negative depends on its [part-of-speech tag](https://www.nltk.org/book/ch05.html). For instance, you might choose to sample nouns or verbs more frequently.\n",
    "- Contextual Similarity-Based Distribution: Develop a distribution that takes into account the similarity of words to the target word. Words that are contextually similar to the target word could have a lower probability of being selected as negative samples.\n",
    "\n",
    "Please implement at least two of the above distributions and compare their performance with the uniform distribution. You can reuse the `train_with_neg_sampling` function to train the model with different negative sampling distributions. Then, please use the `wordsim` function to evaluate the model on the WordSim-353 dataset. Last, briefly summarize your findings and provide possible explanations for the observed results.\n",
    "\n",
    "For students interested in further exploration, consider the following optional directions (10 bonus points):\n",
    "- Combining Multiple Distributions: Explore the effects of using a hybrid approach, combining elements from different distributions.\n",
    "- Dynamic Sampling Strategies: Investigate sampling strategies that change during training, perhaps based on the convergence rate or other learning dynamics.\n",
    "- Corpus-Specific Distributions: Tailor the negative sampling distribution to the specific characteristics of the corpus you are using (e.g., technical vs. general language).\n",
    "- Investigating the Impact on Semantic vs. Syntactic Tasks: Analyze how different sampling distributions affect the model's performance on semantic versus syntactic tasks.\n",
    "\n",
    "You can also explore other negative sampling distributions that are not listed above. Please explain your findings and provide possible explanations for the observed results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO: Write your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: GloVe Embeddings (10 points + 10 bonus points)\n",
    "\n",
    "The second section of this exercise is to leverage pretrained GloVe embeddings to solve the word analogy task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8: Word Analogy Task (10 points)\n",
    "\n",
    "In the word analogy task, we are given three words 'a', 'b', and 'c', and we need to find a word 'd' such that the relationship between 'a' and 'b' is similar to that between 'c' and 'd'. For example, in the analogy \"man is to king as woman is to queen\", the relationship between \"man\" and \"king\" is similar to that between \"woman\" and \"queen\".\n",
    "\n",
    "To solve this, you will use pretrained GloVe embeddings. The general idea is to find the vector representing the relationship between 'a' and 'b', and then search for a word whose relationship with 'c' is most similar to this vector. This similarity is typically measured using cosine similarity.\n",
    "Please implement the `word_analogy` function to find the word 'd' that best completes the analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "import numpy as np\n",
    "\n",
    "# Load Pretrained GloVe Embeddings\n",
    "glove = GloVe(name='6B', dim=100)  # 100-dimensional vectors\n",
    "\n",
    "def word_analogy(a, b, c, embeddings):\n",
    "    \"\"\"\n",
    "    Given words 'a', 'b', 'c', find 'd' such that 'a' is to 'b' as 'c' is to 'd'.\n",
    "    Use cosine similarity to find the closest word.\n",
    "    \n",
    "    :param a, b, c: Words in the analogy a:b :: c:d\n",
    "    :param embeddings: Pretrained GloVe embeddings\n",
    "    :return: The word 'd'\n",
    "    \"\"\"\n",
    "\n",
    "    # Get vectors for the input words\n",
    "    vec_a = embeddings[a]\n",
    "    vec_b = embeddings[b]\n",
    "    vec_c = embeddings[c]\n",
    "\n",
    "    # TODO: Compute the vector for 'd' (vec_a to vec_b is as vec_c to vec_d)\n",
    "\n",
    "    # TODO: Search for the word that is closest to vec_d\n",
    "    # Hint: Iterate over the embeddings and use cosine_similarity\n",
    "\n",
    "    d = None\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are examples to test the word analogy function:\n",
    "- Geographical Analogies: \"France is to Paris as Italy is to ?\"\n",
    "- Grammatical Analogies: \"happy is to happiest as cold is to ?\"\n",
    "- Semantic Analogies: \"cat is to kitten as dog is to ?\"\n",
    "- Syntactic Analogies: \"quick is to quickly as slow is to ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [['france', 'paris', 'italy'], ['happy', 'happiest', 'cold'], ['cat', 'kitten', 'dog'], ['quick', 'quickly', 'slow']]\n",
    "\n",
    "for a, b, c in test_cases:\n",
    "    d = word_analogy(a, b, c, glove)\n",
    "    print(f'{a} is to {b} as {c} is to {d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Debiasing Word Vectors (10 bonus points)\n",
    "\n",
    "Bias in word vectors is a significant issue in natural language processing, leading to stereotypical or prejudiced results in various applications. The paper [*Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings*](https://arxiv.org/abs/1607.06520) presents methods for reducing gender bias in word embeddings. This exercise focuses on implementing two key algorithms from this paper: Neutralization and Equalization.\n",
    "- Neutralization: Remove gender bias from non-gender specific words.\n",
    "- Equalization: Adjust vectors for gender-specific words to ensure equal gender representation.\n",
    "\n",
    "To show how GloVe word embeddings relate to gender, let's examine the relationship between occupation-related words in GloVe embeddings and a constructed \"gender\" vector.\n",
    "The \"gender\" vector is defined as the difference between the GloVe embeddings of several gender-related word pairs, such as \"man - woman\", \"king - queen\", \"mother - father\", \"girl - boy\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_gender_vector(embeddings, gender_pairs):\n",
    "    \"\"\"\n",
    "    Construct a gender vector based on differences of word pairs.\n",
    "    \n",
    "    :param embeddings: GloVe word embeddings.\n",
    "    :param gender_pairs: List of tuples containing gender-specific word pairs.\n",
    "    :return: Constructed gender vector.\n",
    "    \"\"\"\n",
    "    gender_vector = []\n",
    "    for word_a, word_b in gender_pairs:\n",
    "        gender_vector.append(embeddings[word_a] - embeddings[word_b])\n",
    "    return np.mean(gender_vector, axis=0)\n",
    "\n",
    "gender_pairs = [('girl', 'boy')]\n",
    "gender_vector = construct_gender_vector(glove, gender_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can calculate the cosine similarity between the gender vector and the GloVe embeddings of some common occupation-related words.\n",
    "It can be observed that these similarities indeed reflect certain unhealthy gender stereotypes and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_with_gender(word, embeddings, gender_vector):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between a word and the gender vector.\n",
    "    \n",
    "    :param word: Target word.\n",
    "    :param embeddings: GloVe word embeddings.\n",
    "    :param gender_vector: Constructed gender vector.\n",
    "    :return: Cosine similarity.\n",
    "    \"\"\"\n",
    "    word_vector = embeddings[word]\n",
    "    similarity = cosine_similarity(word_vector, gender_vector)\n",
    "    return similarity\n",
    "\n",
    "occupation_words = ['nurse', 'engineer', 'teacher', 'scientist', 'receptionist', 'programmer', 'actor', 'doctor']\n",
    "for word in occupation_words:\n",
    "    similarity = similarity_with_gender(word, glove, gender_vector)\n",
    "    print(f\"Similarity of '{word}' with gender vector: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9: Neutralization (5 bonus points)\n",
    "\n",
    "For a word vector $\\mathbf{w}$ and a gender direction $\\mathbf{g}$, the neutralization is performed by removing the projection of $\\mathbf{w}$ on $\\mathbf{g} $. The equation for neutralization is:\n",
    "$$\n",
    "\\mathbf{w}_{\\text{neutralized}} = \\mathbf{w} - \\frac{\\mathbf{w} \\cdot \\mathbf{g}}{\\|\\mathbf{g}\\|^2} \\mathbf{g}.\n",
    "$$\n",
    "\n",
    "You task is to implement the Neutralization algorithm to remove gender bias from non-gender specific words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word_vector, gender_direction):\n",
    "    \"\"\"\n",
    "    Neutralize bias in the word vector by removing the projection on the gender direction.\n",
    "    \n",
    "    :param word_vector: Vector representation of the word.\n",
    "    :param gender_direction: The gender direction vector.\n",
    "    :return: Debiased word vector.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function to neutralize the word vector\n",
    "    return None\n",
    "\n",
    "word_vector = glove['receptionist']\n",
    "print('Before neutralization, the similarity of \"receptionist\" with gender vector:', cosine_similarity(word_vector, gender_vector))\n",
    "neutralized_vector = neutralize(word_vector, gender_vector)\n",
    "print('After neutralization, the similarity of \"receptionist\" with gender vector:', cosine_similarity(neutralized_vector, gender_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10: Equalization (5 bonus points)\n",
    "\n",
    "The Equalization algorithm is designed to adjust pairs of gender-specific words so that they are equidistant to the gender-neutral words.\n",
    "The procedure involves several steps, including computing the mean of the pair, projecting onto the gender direction, and normalizing the difference.\n",
    "\n",
    "Given a pair of word vectors $\\mathbf{w}_a$ and $\\mathbf{w}_b$ for gender-specific words (e.g., 'actor' and 'actoress'), and the gender direction $\\mathbf{g}$, the equalization adjusts both vectors so that they are equidistant to the bias direction. The key steps involve:\n",
    "\n",
    "1. Calculate the mean vector $$\\mu = \\frac{\\mathbf{w}_a + \\mathbf{w}_b}{2}.$$\n",
    "2. Compute the projections of $\\mu$ over the gender direction:\n",
    "   $$\n",
    "   \\begin{align}\n",
    "      \\mu_B &= \\frac{\\langle \\mu, \\mathbf{g} \\rangle}{\\| \\mathbf{g} \\|^2} \\mathbf{g}, \\\\\n",
    "      \\mu_{\\perp} &= \\mu - \\mu_B.\n",
    "   \\end{align}\n",
    "   $$\n",
    "3. Adjust the bias parts of $\\mathbf{w}_{aB}$ and $\\mathbf{w}_{bB}$:\n",
    "   $$\n",
    "   \\begin{align}\n",
    "      \\mathbf{w}_{aB}^{\\text{corrected}} &= \\sqrt{ \\left| 1 - \\| \\mu_{\\perp} \\|^2 \\right| } \\times \\frac{\\mathbf{w}_{aB} - \\mu_B}{|(\\mathbf{w}_{a} - \\mu_{\\perp}) - \\mu_B|}, \\\\\n",
    "      \\mathbf{w}_{bB}^{\\text{corrected}} &= \\sqrt{ \\left| 1 - \\| \\mu_{\\perp} \\|^2 \\right| } \\times \\frac{\\mathbf{w}_{bB} - \\mu_B}{|(\\mathbf{w}_{b} - \\mu_{\\perp}) - \\mu_B|}.\n",
    "   \\end{align}\n",
    "   $$\n",
    "4. Combine the corrected bias parts with $\\mu_{\\perp}$:\n",
    "   $$\n",
    "   \\begin{align}\n",
    "     \\mathbf{w}_{a}^{\\text{corrected}} &= \\mathbf{w}_{aB}^{\\text{corrected}} + \\mu_{\\perp}, \\\\\n",
    "     \\mathbf{w}_{b}^{\\text{corrected}} &= \\mathbf{w}_{bB}^{\\text{corrected}} + \\mu_{\\perp}.\n",
    "   \\end{align}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(w_a, w_b, g):\n",
    "    \"\"\"\n",
    "    Equalize the word vectors for a pair of gender-specific words.\n",
    "    \n",
    "    :param w_a, w_b: Word vectors for the gender-specific pair.\n",
    "    :param g: The gender direction vector.\n",
    "    :return: Equalized word vectors for the pair.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function to equalize the word vectors\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_a, w_b = glove['actor'], glove['actress']\n",
    "equalized_vectors = equalize(w_a, w_b, gender_vector)\n",
    "print('Cosine similarity before equalization:')\n",
    "print(f'  {cosine_similarity(w_a, gender_vector)} (actor)')\n",
    "print(f'  {cosine_similarity(w_b, gender_vector)} (actress)')\n",
    "print('Cosine similarity after equalization:')\n",
    "print(f'  {cosine_similarity(equalized_vectors[0], gender_vector)} (actor)')\n",
    "print(f'  {cosine_similarity(equalized_vectors[1], gender_vector)} (actress)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
